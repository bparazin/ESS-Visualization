{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I'm importing the gauge data from the 2 USGS tab-separated files I've got and the waterwatch website, as well as organizing it into a useful form. This is all going to be very-specifcally set up for my specific instance just because I really don't forsee using it for anything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3 as url\n",
    "import re\n",
    "import datetime as dt\n",
    "import pickle\n",
    "http = url.PoolManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This first one pulls each gauge number and it's lat lon coords\n",
    "gauge_filepath = r'C:\\Users\\bpara\\Geo data\\Site coordinates.txt'\n",
    "raw_coords = open(gauge_filepath, 'r')\n",
    "gauge_final = dict()\n",
    "first_line = True\n",
    "for x in raw_coords:\n",
    "    if first_line: \n",
    "        first_line = False\n",
    "        continue\n",
    "    if not x.split()[0] in gauge_final:\n",
    "        temp = {\"lat\":x.split(\"\t\")[7], \"lon\":x.split(\"\t\")[8]}\n",
    "        gauge_final[x.split()[0]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"USGS Gauge data\", 'rb') as f:\n",
    "        gauge_final = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0:02:02.872666\n",
      "Done in 0:01:49.664138\n"
     ]
    }
   ],
   "source": [
    "#And this one gets the time series for flooding and the flood stages (if avaliable) for them\n",
    "for gauge in gauge_final.keys():\n",
    "    t1 = dt.datetime.now()\n",
    "    if 'hgt' in gauge_final[gauge]: continue\n",
    "    site_num = gauge\n",
    "    site = http.request('GET', \"https://nwis.waterdata.usgs.gov/usa/nwis/uv/?referred_module=sw&search_site_no=\"+site_num+\"&search_site_no_match_type=exact&index_pmcode_00065=1&group_key=NONE&sitefile_output_format=html_table&column_name=agency_cd&column_name=site_no&column_name=station_nm&range_selection=date_range&begin_date=2019-01-01&end_date=2019-12-31&format=rdb&date_format=YYYY-MM-DD&rdb_compression=value&list_of_search_criteria=search_site_no%2Crealtime_parameter_selection\")\n",
    "    time_list = []\n",
    "    val_list = []\n",
    "    list_1 = re.split(\"[\\n]\", site.data.decode(\"utf-8\"))\n",
    "    for line in list_1:\n",
    "        if line[0:4] == \"USGS\":\n",
    "            list_2 = line.split(\"\\t\")\n",
    "            val_list.append(list_2[4])\n",
    "            time_list.append(dt.datetime.strptime(list_2[2], '%Y-%m-%d %H:%M'))\n",
    "    gauge_final[gauge][\"time\"] = time_list.copy()\n",
    "    gauge_final[gauge][\"hgt\"] = val_list.copy()\n",
    "    site2 = http.request('GET', \"https://waterwatch.usgs.gov/webservices/floodstage?format=json&site=\" + site_num)\n",
    "    gauge_final[gauge][\"flood\"] = site2.data.decode(\"utf-8\")\n",
    "    file = open(\"USGS Gauge data\", \"wb\")\n",
    "    pickle.dump(gauge_final, file)\n",
    "    file.close()\n",
    "    print(\"Done in \" + str(dt.datetime.now() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
